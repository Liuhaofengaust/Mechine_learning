{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d69325",
   "metadata": {},
   "source": [
    "### Numpy实现深度学习 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d536de2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-04T07:38:57.516812Z",
     "start_time": "2022-10-04T07:38:56.881211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 loss 739.2073147947099\n",
      "199 loss 517.6630567376892\n",
      "299 loss 363.6023778696177\n",
      "399 loss 256.37665666432576\n",
      "499 loss 181.6855046119186\n",
      "599 loss 129.61528282896768\n",
      "699 loss 93.28684071552232\n",
      "799 loss 67.92223190307574\n",
      "899 loss 50.199910009585416\n",
      "999 loss 37.80877091620629\n",
      "1099 loss 29.13940669950568\n",
      "1199 loss 23.070141662992484\n",
      "1299 loss 18.81860966371166\n",
      "1399 loss 15.838699205116365\n",
      "1499 loss 13.74893350264217\n",
      "1599 loss 12.282652789408605\n",
      "1699 loss 11.253332234278236\n",
      "1799 loss 10.53041700305324\n",
      "1899 loss 10.02247177822683\n",
      "1999 loss 9.665421718444945\n",
      "Result: y = 0.029590472953437197 + 0.8487986416211696 x + -0.0051048489720632185 x^2 + -0.0922006800452631 x^3\n"
     ]
    }
   ],
   "source": [
    "# Numpy是通用数值计算的n为数组\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 生成x和y\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 随机权重\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    # 前向传播，计算预测的y\n",
    "    y_pred = a+b*x+ c*x**2+ d*x**3\n",
    "    \n",
    "    # 损失函数\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t%100 == 99:\n",
    "        print(t,\"loss\", loss)\n",
    "        \n",
    "    # 反向计算梯度   ?????\n",
    "    grad_y_pred = 2 * (y_pred - y)\n",
    "    grad_a      = grad_y_pred.sum()\n",
    "    grad_b      = (grad_y_pred * x).sum()\n",
    "    grad_c      = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d      = (grad_y_pred * x ** 3).sum()\n",
    "    \n",
    "    # 更新权重\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "    \n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf874c1",
   "metadata": {},
   "source": [
    "### Pytorch实现深度学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4106b4d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-04T10:48:33.703846Z",
     "start_time": "2022-10-04T10:48:33.269348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "99 loss:  710.80517578125\n",
      "a.grad: tensor(7933.1655)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "199 loss:  457.4346923828125\n",
      "a.grad: tensor(-1386.6780)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "299 loss:  303.76202392578125\n",
      "a.grad: tensor(-752.0720)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "399 loss:  203.80812072753906\n",
      "a.grad: tensor(-1.6655)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "499 loss:  137.7450714111328\n",
      "a.grad: tensor(52.7495)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "599 loss:  94.07067108154297\n",
      "a.grad: tensor(7.0815)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "699 loss:  65.19110107421875\n",
      "a.grad: tensor(-2.7802)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "799 loss:  46.094505310058594\n",
      "a.grad: tensor(-0.8659)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4ba070dde811>:25: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(\"loss.grad:\", loss.grad)\n",
      "<ipython-input-11-4ba070dde811>:29: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(\"loss.grad:\", loss.grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899 loss:  33.46678924560547\n",
      "a.grad: tensor(0.0816)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "999 loss:  25.116777420043945\n",
      "a.grad: tensor(0.0719)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "1099 loss:  19.59528923034668\n",
      "a.grad: tensor(0.0037)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "1199 loss:  15.944252967834473\n",
      "a.grad: tensor(-0.0063)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "1299 loss:  13.529943466186523\n",
      "a.grad: tensor(-0.0045)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "1399 loss:  11.933515548706055\n",
      "a.grad: tensor(-0.0039)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "1499 loss:  10.877869606018066\n",
      "a.grad: tensor(-0.0024)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "1599 loss:  10.179801940917969\n",
      "a.grad: tensor(4.1917e-05)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "1699 loss:  9.718208312988281\n",
      "a.grad: tensor(0.0025)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "1799 loss:  9.412985801696777\n",
      "a.grad: tensor(0.0039)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "1899 loss:  9.211148262023926\n",
      "a.grad: tensor(0.0043)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "1999 loss:  9.077690124511719\n",
      "a.grad: tensor(0.0042)\n",
      "x.grad: None\n",
      "loss.grad: None\n",
      "loss.grad: None\n",
      "Result: y=2.5859369756631168e-08+0.8410506248474121+-9.709429704685135e-09x^2+-0.0910985916852951x^3\n"
     ]
    }
   ],
   "source": [
    "import torch, math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "print(a.grad)             # a 刚开始也没有梯度,等loss.backward()之后\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = a+b*x+ c*x**2+ d*x**3\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t%100 == 99:\n",
    "        print(t, \"loss: \", loss.item())\n",
    "        print(\"a.grad:\", a.grad)\n",
    "        print(\"x.grad:\", x.grad)         # x没有梯度\n",
    "        print(\"loss.grad:\", loss.grad)\n",
    "        \n",
    "    loss.backward()   # 计算谁的梯度？？？ abcd参数的梯度\n",
    "    if t%100 == 99:\n",
    "        print(\"loss.grad:\", loss.grad)   # loss没有梯度，只是对参数求导\n",
    "    \n",
    "    # abcd梯度关闭\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        \n",
    "        # a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "        \n",
    "print(f'Result: y={a.item()}+{b.item()}+{c.item()}x^2+{d.item()}x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89bc648",
   "metadata": {},
   "source": [
    "### 定义新的Autograd函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5*input**2 -1)\n",
    "    \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "    \n",
    "    y_pred = a+ b*P3(c + d*x)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74ce82e",
   "metadata": {},
   "source": [
    "### 使用优化器更新权重 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb5494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-04T12:36:01.321964Z",
     "start_time": "2022-10-04T12:36:00.792189Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# 使用简单的序惯模型\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 优化器，将模型的参数传进去，优化器进行梯度下降，更新参数\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "       \n",
    "    if t%100 == 99:\n",
    "        print(t, loss.item())\n",
    "           \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    # 开始梯度下降，更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(\"model: \", model)\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b8c87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-04T12:56:11.575160Z",
     "start_time": "2022-10-04T12:56:11.572649Z"
    }
   },
   "source": [
    "### 自定义nn模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec1514",
   "metadata": {},
   "source": [
    "### Pytorch:控制流+权重共享\n",
    "* 为了展示 PyTorch 动态图的强大功能，我们将实现一个非常奇怪的模型：一个从三到五阶（动态变化）的多项式，在每次正向传递中选择一个 3 到 5 之间的一个随机数，并将这个随机数作为阶数，第四和第五阶共用同一个权重来多次重复计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ece890f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-04T13:09:30.736959Z",
     "start_time": "2022-10-04T13:09:30.729152Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-25-6b8214825c5e>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [25]\u001b[0;36m\u001b[0m\n\u001b[0;31m    for t in range(30000):\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import random, torch, math\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(self)\n",
    "        self.a = torch.nn.Parameter(torch.randn())\n",
    "        self.b = torch.nn.Parameter(torch.randn())\n",
    "        self.c = torch.nn.Parameter(torch.randn())\n",
    "        self.d = torch.nn.Parameter(torch.randn())\n",
    "        self.e = torch.nn.Parameter(torch.randn())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.a + self.b * x + self.c * x**2 + self.d * x**3\n",
    "        \n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "    \n",
    "    def string(self):\n",
    "        return f\"y = {self.a.item()}+ {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?\"\n",
    "    \n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "model = DynamicNet()\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction=\"sim\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "\n",
    "for t in range(30000):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t%2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f\"Result: {model.string()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea71bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
